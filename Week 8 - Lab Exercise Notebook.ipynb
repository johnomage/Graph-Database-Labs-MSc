{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Technologies for Anti-Money Laundering and Financial Crime**\n",
        "# Lab Exercise 6 - March 2023\n",
        "### *COMP-1831-M01-2022-23*\n",
        "\n",
        "Welcome to this lab exercise on unsupervised machine learning techniques for fraud detection. In today's session, you will be introduced to a fundamental concept in machine learning - unsupervised learning, and how it can be applied in the domain of fraud detection. Fraud is a critical issue in the financial industry, and detecting fraudulent activities is essential to protect businesses and customers from financial loss.\n"
      ],
      "metadata": {
        "id": "unRyXBQWM47T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset \n",
        "\n",
        "To start this lab session, we'll need to download the dataset into our notebook environment using a tool called gdown. This will allow us to access the data and begin exploring it using the unsupervised machine learning techniques we'll be learning today. \n",
        "\n",
        "Link to dataset: https://drive.google.com/file/d/1qE58w_PEfg-qIICSX093eVK0Po6Pp0pW/view?usp=sharing\n",
        "\n",
        "Recall that the exclamation mark (`!`) in front of a command indicates a bash command."
      ],
      "metadata": {
        "id": "wcx7Eu0aPfUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "MqzKRpzEkOy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"1qE58w_PEfg-qIICSX093eVK0Po6Pp0pW\""
      ],
      "metadata": {
        "id": "dlz8Mjn0QaeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⬅️ Your should be able to see a new file called `CreditCardTxs_PCA_Dataset.zip` in your file explorer.\n",
        "\n",
        "Since this is a `zip` archive, go ahead and extract the csv dataset by running the command below:"
      ],
      "metadata": {
        "id": "-St5i_qkSHnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"CreditCardTxs_PCA_Dataset.zip\""
      ],
      "metadata": {
        "id": "S7bOWkOcTVDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "\n",
        "The dataset we'll be using in this lab contains credit card transactions made by European cardholders in September 2013. This dataset covers two days and includes 492 frauds out of a total of 284,807 transactions, making it a highly unbalanced dataset where only 0.172% of transactions are fraudulent.\n",
        "\n",
        "The dataset only includes numerical input variables that have been transformed using Principal Component Analysis (PCA). Due to confidentiality issues, the original features or any additional background information about the data is not provided. The principal components obtained through PCA are labeled as features V1, V2, ..., V28. However, the features that have not been transformed with PCA are 'Time' and 'Amount'.\n",
        "\n",
        "**Time:** Indicates the time elapsed between each transaction and the first transaction in the dataset, while the\n",
        "\n",
        "**Amount:**: Indicates the transaction amount. The 'Class' feature is the response variable, which takes the value 1 in the case of fraud and 0 otherwise.\n"
      ],
      "metadata": {
        "id": "jieXB3EZUQ5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: PCA and EDA"
      ],
      "metadata": {
        "id": "Lz3XNrJ-P17m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) is an **unsupervised** statistical technique used to transform a large set of variables into a smaller set of uncorrelated variables, known as principal components. It is often used as a data reduction technique to simplify complex datasets, while preserving most of the important information.\n",
        "\n",
        "Let's consider an example dataset of sales data for a chain of stores. We have information about the sales of different products (e.g., groceries, electronics, clothing) for each store in the chain. We also have data on other factors that may influence sales, such as store location, size, and customer demographics. The dataset may have dozens or even hundreds of variables, making it difficult to analyze and interpret.\n",
        "\n",
        "To apply PCA to this dataset, we would first standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all variables are on the same scale and have equal weight in the analysis.\n",
        "\n",
        "Next, we calculate the covariance matrix, which measures the degree to which each variable is correlated with every other variable. The diagonal elements of the covariance matrix represent the variances of each variable, while the off-diagonal elements represent the covariances between pairs of variables.\n",
        "\n",
        "The eigenvectors and eigenvalues of the covariance matrix are then calculated. The eigenvectors represent the directions in which the data vary the most, while the eigenvalues represent the amount of variance explained by each eigenvector. The eigenvectors are sorted in order of decreasing eigenvalue, and the first few eigenvectors (i.e., those with the highest eigenvalues) are retained as principal components.\n",
        "\n",
        "These principal components are then used to transform the original dataset into a new set of variables that are uncorrelated with each other. This can simplify the dataset and make it easier to analyze. The new variables are linear combinations of the original variables, weighted by the eigenvectors.\n",
        "\n",
        "Here's an example of how to perform PCA in Python using the popular scikit-learn library:"
      ],
      "metadata": {
        "id": "wbNKtdjDW596"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Standardize the data\n",
        "X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Create a PCA object and fit the data\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "\n",
        "# Print the explained variance ratios of the principal components\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot the transformed data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wjZG2A3aXffr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the iris dataset, which contains measurements of the sepal length, sepal width, petal length, and petal width for three species of iris flowers. We standardize the data and then create a PCA object with two principal components. We fit the PCA model to the standardized data and transform the data using the fitted model. Finally, we plot the transformed data using the first two principal components.\n",
        "\n",
        "The explained_variance_ratio_ attribute of the PCA object tells us the proportion of variance explained by each principal component. In this case, the first principal component explains **72.77%** of the variance, while the second principal component explains **23.03%** of the variance. Together, the two principal components explain 95.80% of the variance, which suggests that the original four variables are highly correlated.\n",
        "\n",
        "![image.gif](https://i.stack.imgur.com/Q7HIP.gif)\n",
        "\n",
        "For better understanding here is a very good visual explanation of PCA: https://setosa.io/ev/principal-component-analysis/\n",
        "\n",
        "### PCA for Anonymization\n",
        "\n",
        "If we were to remove the dimensionality reduction element of PCA, it would only perform a rotation of the original data space.\n",
        "\n",
        "This property of PCA makes it useful for data masking and anonymisation. Although the original features cannot be provided due to confidentiality concerns, PCA allows us to preserve the distances between data points and maintain the structure of the original points. This means that the data remains useful for analysis purposes while protecting the privacy of individuals in the dataset.\n",
        "\n",
        "This is why PCA was used in this dataset in the first place - to address the confidentiality concerns associated with the data."
      ],
      "metadata": {
        "id": "_be2eeBmX3H2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1.1: PCA on Wine dataset**\n",
        "Use the code template below to perform PCA on the wine dataset provided by `sklearn.datasets`. Extract the first three principal components and print their explained variance ratios.\n",
        "\n",
        "Use the first two principal components to display a scatter plot with color coded classes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lp1rCC0rKZ_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the iris dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "#### Step 1: Standardize the data\n",
        "# <Your Code Here>\n",
        "\n",
        "#### Create a PCA object and fit the data\n",
        "# <Your Code Here>\n",
        "\n",
        "#### Print the explained variance ratios of the principal components\n",
        "# <Your Code Here>\n",
        "\n",
        "# Plot the transformed data\n",
        "# <Your Code Here>"
      ],
      "metadata": {
        "id": "kd0ESvxHLWii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fraud Detection with Unsupervised Learning\n"
      ],
      "metadata": {
        "id": "ygzddMmHdV2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python code dependencies:"
      ],
      "metadata": {
        "id": "-PRpWPpHb4UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "P0GtGh4Db1AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset\n",
        "\n",
        "Let's go ahead and lead the CSV dataset as a DataFrame to starting working on our analysis."
      ],
      "metadata": {
        "id": "Z2a80eVHWS-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./creditcard.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "in59HWNEWvGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "As always, it's important to understand the structure and content of the dataset we are working with so can proceed with a bery basic exploratory data analysis. "
      ],
      "metadata": {
        "id": "cuIc9tcMc3aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "dWCfxaM5crI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "xR5YWocRcxmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Imbalance \n",
        "It's common in fraud detection to work with imbalanced datasets, thus we need to quantify how large the imbalance is."
      ],
      "metadata": {
        "id": "vCX8zhaldgyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_per_class = df.Class.value_counts()\n",
        "count_per_class.plot(kind=\"bar\")\n",
        "print(count_per_class)\n",
        "print(\"Fraud cases are {0:.2f}% of the dataset\".format(count_per_class[1]/count_per_class[0]*100))"
      ],
      "metadata": {
        "id": "f1tYee9JdiF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class over Time \n",
        "\n",
        "As mentioned above, Time is one of the two features which has not been masked and therehore holds some semantic meaning. We can explore that further to see if there is a correlation with our `Class` label. "
      ],
      "metadata": {
        "id": "MyKOirQWfRs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n",
        "\n",
        "fraud_transactions = df.Time[df.Class == 1]\n",
        "normal_transactions = df.Time[df.Class == 0]\n",
        "\n",
        "ax1.hist(fraud_transactions, bins = 50, edgecolor=\"black\")\n",
        "ax1.set_xlim([min(fraud_transactions), max(fraud_transactions)])\n",
        "ax1.set_title('Fraudulent Transactions', fontsize=15)\n",
        "ax1.set_ylabel(\"Number of Transactions\",  fontsize=13)\n",
        "\n",
        "ax2.hist(normal_transactions, bins = 50, edgecolor=\"black\")\n",
        "ax2.set_xlim([min(normal_transactions), max(normal_transactions)])\n",
        "ax2.set_title('Normal Transactions',  fontsize=15)\n",
        "\n",
        "ax2.set_xlabel('Time (in Seconds)',  fontsize=13)\n",
        "ax2.set_ylabel('Number of Transactions',  fontsize=13)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bafW5xS5fuT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering\n",
        "\n",
        "Create an \"Hour\" column which holds the hour in a 24 hour span"
      ],
      "metadata": {
        "id": "0Y-ZwYqQifr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timedelta = pd.to_timedelta(df['Time'], unit='s')\n",
        "df['Hour'] = (timedelta.dt.components.hours).astype(int)"
      ],
      "metadata": {
        "id": "YdKFdwozieFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Correlation of the PCA components"
      ],
      "metadata": {
        "id": "Shu5KgIwhXsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = list(df.iloc[:,1:15].columns) + [\"Hour\"]\n",
        "\n",
        "frauds = df.Class == 1\n",
        "normals = df.Class == 0\n",
        "\n",
        "grid = gridspec.GridSpec(14, 2)\n",
        "plt.figure(figsize=(20,20*4))\n",
        "\n",
        "for n, col in enumerate(df[columns]):\n",
        "    ax = plt.subplot(grid[n])\n",
        "    sns.distplot(df[col][frauds], color='g', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=0.6)) \n",
        "    sns.distplot(df[col][normals],color='r', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=0.6))\n",
        "    ax.set_ylabel('Density', fontsize=13)\n",
        "    ax.set_title(str(col), fontsize=20)\n",
        "    ax.set_xlabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5mvKrB9lh-xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the observed features exhibit a significant level of selectivity with regards to the distribution of the two distinct values of the dependent variable, \"Class\".\n",
        "\n",
        "More precisely, V1, V2, V3, V4, V11, V12, and especially V14 features' distributions seem to differ with regards to the Class label value. As a result, they are considered good features for modelling.\n"
      ],
      "metadata": {
        "id": "4uzJhoZgjZi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2.1** \n",
        "\n",
        "In the above, section we've plotted histogram distributions for the first 14 features of the dataset. Plot the histogram distributions for the remaining features (V15-V28) and identify features that can be important for fraud classification. "
      ],
      "metadata": {
        "id": "c5-uOxzZo_Wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Modelling"
      ],
      "metadata": {
        "id": "1O0bydtAqWnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "K-means clustering is a popular technique in unsupervised machine learning used to group similar data points together. The algorithm works by taking a dataset and dividing it into k clusters, where k is a user-specified number of clusters.\n",
        "\n",
        "The K-means algorithm starts by randomly selecting k points from the dataset to act as initial cluster centers. It then iteratively assigns each data point in the dataset to the nearest cluster center based on the Euclidean distance between the data point and the cluster center. Once all data points have been assigned to a cluster, the algorithm computes the new cluster centers as the mean of all the data points within that cluster.\n",
        "\n",
        "The algorithm continues to iterate between assigning data points to clusters and computing new cluster centers until the clusters stop changing or a maximum number of iterations is reached. At the end of the algorithm, each data point belongs to one of the k clusters.\n",
        "\n",
        "One important consideration when using K-means is choosing the value of k. This can be done using techniques such as the elbow method or silhouette analysis, which help to identify the optimal number of clusters based on the characteristics of the data.\n",
        "\n",
        "![image.gif](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif?20170530143526)\n"
      ],
      "metadata": {
        "id": "Y0LOiEpOxvcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on our preliminary significance analysis of the features, let's take the next step and identify a pair of significant features that can be used to classify transactions using clustering. With clustering we can identify natural groupings of transactions within the dataset, which in turn can help us better understand the underlying patterns and structures in the data. Using just a pair of features might not be an optimal choice but it will help us understand how the algorithm works by visualizing the results.\n",
        "\n",
        "On top of that, let's see if the natural groupings that derived from the dataset features align with the corresponding Class of transactions."
      ],
      "metadata": {
        "id": "K21KTLmQFbAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_kmeans_on_features(X, y):\n",
        "  kmeans = KMeans(n_clusters=2, max_iter=3000, n_init=20)\n",
        "\n",
        "  y_pred_kmeans = kmeans.fit_predict(X)\n",
        "  accuracy = accuracy_score(y, y_pred_kmeans)\n",
        "  print(\"{:.3f}%\".format(accuracy*100))\n",
        "  print(classification_report(y, y_pred_kmeans))\n",
        "  return y_pred_kmeans"
      ],
      "metadata": {
        "id": "R8X1tMkuGfq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_pair = ['V14', 'V12']\n",
        "\n",
        "X = df[feature_pair]\n",
        "y = df[[\"Class\"]]\n",
        "\n",
        "y_pred_kmeans = apply_kmeans_on_features(X, y)"
      ],
      "metadata": {
        "id": "kOJMylQsx8RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly with just two features V14 and V12 we can get decent accuracy (86%) with K-Means clustering. \n",
        "\n",
        "**Note:** If accuracy is displayed as 13% this is because the KMeans algorithms is not really doing a semantic classification. Instead it just splits the dataset into two clusters and therefore in some cases the clusters corresponding to the classes are in reverse. To fix that we can just flip the cluster labels or just re-run K-Means.\n",
        "\n",
        "Also recall that accuracy metric by itself is not sufficient to understand the performance of a model."
      ],
      "metadata": {
        "id": "L0u2VA8bG-Cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter plots - Original vs Predictions"
      ],
      "metadata": {
        "id": "93FW2ztiI52B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax =plt.subplots(1,2, figsize=(15, 6))\n",
        "sns.scatterplot(x=X[feature_pair[0]], y=X[feature_pair[1]], hue=df['Class'], ax=ax[0])\n",
        "sns.scatterplot(x=X[feature_pair[0]], y=X[feature_pair[1]], hue=y_pred_kmeans, ax=ax[1])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QPT0lGV00bYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply the same clustering process but this time with a different pair of features (V11, V12). Based on our analysis above this pair should yield relatively good results as well. "
      ],
      "metadata": {
        "id": "GpZ6GAetMsUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_pair = ['V11', 'V12']\n",
        "\n",
        "X = df[feature_pair]\n",
        "y = df[[\"Class\"]]\n",
        "\n",
        "y_pred_kmeans = apply_kmeans_on_features(X, y)"
      ],
      "metadata": {
        "id": "8euPAWhgMvQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter plots - Original vs Predictions"
      ],
      "metadata": {
        "id": "vdv5cnxPNxME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax =plt.subplots(1,2, figsize=(15, 6))\n",
        "sns.scatterplot(x=X[feature_pair[0]], y=X[feature_pair[1]], hue=df['Class'], ax=ax[0])\n",
        "sns.scatterplot(x=X[feature_pair[0]], y=X[feature_pair[1]], hue=y_pred_kmeans, ax=ax[1])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "GHVaK-afNODl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion \n",
        "Unfortunately, the accuracy level we observed for this new pair of features (which we thought would be similar to the previous pair V12, V14) is actually quite low. As we can see from the scatter plots, the K-means algorithm did not split the clusters very well. One reason for this is that K-means assumes that the dataset classes are balanced, which is not the case for our current dataset.\n",
        "\n",
        "To address this class imbalance issue, we will employ a technique called Synthetic Minority Oversampling TEchnique (SMOTE), which we learned about in a previous lab. SMOTE is a powerful technique that can help us balance our dataset by generating synthetic samples for the minority class. By doing so, we can improve the accuracy of our models and obtain more meaningful insights from our data. Let's go ahead and apply SMOTE to our dataset and see if it can help us improve our clustering results!"
      ],
      "metadata": {
        "id": "nyqkigE5Nyx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tackle class imbalance with SMOTE"
      ],
      "metadata": {
        "id": "iSYo6gXgO4a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Original dataset shape : {y.value_counts()}')\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "print(f'Resampled dataset shape {y_res.value_counts()}')"
      ],
      "metadata": {
        "id": "2Ci8gIHg5qmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_kmeans = apply_kmeans_on_features(X_res, y_res)"
      ],
      "metadata": {
        "id": "7mS-Dvpb6gT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax =plt.subplots(1,2, figsize=(15, 6))\n",
        "sns.scatterplot(x=X_res[feature_pair[0]], y=X_res[feature_pair[1]], hue=y_res.Class, ax=ax[0])\n",
        "sns.scatterplot(x=X_res[feature_pair[0]], y=X_res[feature_pair[1]], hue=y_pred_kmeans, ax=ax[1])\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "-8ZbwH2n6id_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3.1: K-Means clustering**\n",
        "\n",
        "Taking into consideration the feature analysis performed in exercise 2.1, use another pair of features (V1-V28) to run k-means clustering and perform predictions. Justify your feature pair decision and evaluate your predictions accuracy.\n",
        "\n",
        "For the same feature pair, run SMOTE on the dataset and re-run K-means clustering, perform predictions and re-evaluate the accuracy. \n",
        "\n",
        "Explain any differences in the classification report."
      ],
      "metadata": {
        "id": "X67WYbAR4Ogt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <Your code here>"
      ],
      "metadata": {
        "id": "HUeBbGU6RnwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3.2: 3D scaterplot**\n",
        "\n",
        "Select any three input features of the dataset to plot a 3d scatterplot of the points in 3D space. Use the \"Class\" column to color code the data points.\n",
        "\n",
        "Perform K-means clustering on the three features that you selected above and plot again the 3d scatterplot but this time use the K-means cluster values for color coding.\n",
        "\n",
        "You can use the template below to do this exercise."
      ],
      "metadata": {
        "id": "I_wcykQrlwmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X = # Your code here\n",
        "y = # Your code here\n",
        "\n",
        "# axes instance\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax = Axes3D(fig, auto_add_to_figure=False)\n",
        "fig.add_axes(ax)\n",
        "\n",
        "# plot\n",
        "sc = ax.scatter(X.iloc[:,0],X.iloc[:,1],X.iloc[:,2], s=40, c=y, marker='o', alpha=1)\n",
        "ax.set_xlabel('X Label')\n",
        "ax.set_ylabel('Y Label')\n",
        "ax.set_zlabel('Z Label')\n",
        "\n",
        "# legend\n",
        "plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n",
        "\n",
        "# save\n",
        "plt.savefig(\"scatter_hue\", bbox_inches='tight')"
      ],
      "metadata": {
        "id": "1bz85j6IlyLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isolation Forest\n",
        "\n",
        "Isolation Forest is an anomaly detection algorithm that uses a tree-based approach to identify outliers in a dataset. The algorithm works by randomly selecting a feature and a split value within the feature range to create a partition of the data. This process is repeated recursively to create a tree-like structure until all instances are isolated.\n",
        "\n",
        "The number of splits required to isolate a given data point is considered to be a measure of its normality. An outlier is identified as an instance that requires fewer splits to be isolated, meaning it is less normal than other instances in the dataset. The algorithm can also be used to identify multiple outliers simultaneously.\n",
        "\n",
        "One of the main advantages of Isolation Forest is that it is able to handle datasets with a high dimensionality and does not require prior knowledge of the data distribution. This makes it a useful tool in a wide range of applications, such as fraud detection, network intrusion detection, and anomaly detection in sensor data.\n",
        "\n",
        "However, it is important to note that Isolation Forest is not perfect and may not be able to detect all anomalies in some datasets. It is also possible for the algorithm to be sensitive to certain parameters, such as the number of trees in the forest or the subsampling rate.\n",
        "\n",
        "Overall, Isolation Forest is a powerful algorithm for identifying anomalies in large and complex datasets. Its ability to handle high-dimensional data and lack of assumptions about data distribution make it a useful tool for data scientists and analysts.\n",
        "\n",
        "![image.gif](https://unit8.com/wp-content/uploads/2021/07/1_67DU32noLb5ZAYAD_YdfxA.gif)\n"
      ],
      "metadata": {
        "id": "EPRz5foK8PKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isf_preds_relabel(y_pred):\n",
        "  y_pred[y_pred == 1] = 0\n",
        "  y_pred[y_pred == -1] = 1\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "_LtCsKLw_HcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Isolation Forest model using a subset of the dataset features"
      ],
      "metadata": {
        "id": "xhKnLJSQSHIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v1_to_v14_features = [f\"V{i}\" for i in range(1,10)]\n",
        "X = df[v1_to_v14_features]\n",
        "y = df.Class \n",
        "\n",
        "X.info()"
      ],
      "metadata": {
        "id": "tPHvsC1yCTD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into train/test 80%/20% respectively."
      ],
      "metadata": {
        "id": "xGUX6lqRSb3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)"
      ],
      "metadata": {
        "id": "UeQKgI7xScBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ISF = IsolationForest(random_state=42, max_samples=1.0)\n",
        "ISF.fit(X_train)\n",
        "\n",
        "y_pred = ISF.predict(X_test)\n",
        "y_pred = isf_preds_relabel(y_pred)"
      ],
      "metadata": {
        "id": "e-3jPwjF_M8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "EApjEJkX_51X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "sns.set(font_scale=3)\n",
        "confusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
        "plt.title(\"Confusion matrix\", fontsize=22)\n",
        "plt.ylabel('True label', fontsize=20)\n",
        "plt.xlabel('Clustering label', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q6VgCiy5THg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.3: Isolation Forest\n",
        "\n",
        "Try to improve the overall performance of the model (pay attention on the macro avg accuracy. Re-run IsolationForest algorithm on a different subset of features and display the classification report and confusion matrix."
      ],
      "metadata": {
        "id": "sLYwYx3IrjNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "Iln1q-oJsLcV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}